# Copy this file to .env and fill in your values. Do not commit .env.

# --- LLM provider (choose one) ---
# Options: openai | openrouter | ollama
# Judges and VisionInspector use the same provider.
LLM_PROVIDER=openai

# OpenAI (when LLM_PROVIDER=openai) — paid
# OPENAI_API_KEY=sk-...
# OPENAI_MODEL=gpt-4o-mini
# OPENAI_VISION_MODEL=gpt-4o-mini

# OpenRouter (when LLM_PROVIDER=openrouter) — free tier available
# Get key at https://openrouter.ai/keys
# OPENROUTER_API_KEY=sk-or-...
# OPENROUTER_MODEL=openai/gpt-4o-mini
# OPENROUTER_VISION_MODEL=openai/gpt-4o
# OPENROUTER_API_BASE=https://openrouter.ai/api/v1

# Ollama local (when LLM_PROVIDER=ollama) — free, no key
# Run: ollama serve (default http://localhost:11434)
# ollama pull llama3.2 && ollama pull llava  # for vision
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama3.2
# OLLAMA_VISION_MODEL=llava

# --- Vision: optional provider override (defaults to LLM_PROVIDER) ---
# Use Gemini for vision only: VISION_PROVIDER=gemini + GOOGLE_API_KEY (+ GOOGLE_VISION_MODEL)
# VISION_PROVIDER=gemini
# GOOGLE_API_KEY=...
# GOOGLE_VISION_MODEL=gemini-2.0-flash

# --- Multi-model  (role-based: set these to use Groq + Gemini + OpenAI per role) ---
# When set, overrides single LLM_PROVIDER for that role. LangSmith (below) still records the full reasoning loop.
# Judicial bench (Prosecutor, Defense, Tech Lead): Groq for speed
# JUDICIAL_PROVIDER=groq
# GROQ_API_KEY=...
# GROQ_JUDICIAL_MODEL=llama-3.3-70b-versatile
# Vision & PDF detective (VisionInspector, DocAnalyst): Gemini for large context and diagram analysis
# DETECTIVE_PROVIDER=gemini
# GOOGLE_DETECTIVE_MODEL=gemini-2.5-pro
# Forensic investigator (optional RepoInvestigator code validation): OpenAI for structured output
# FORENSIC_PROVIDER=openai
# OPENAI_FORENSIC_MODEL=gpt-4o-mini

# --- Optional: vision inspection (diagram analysis) ---
# AUDITOR_RUN_VISION=1   # set to run VisionInspector when PDF has images

# --- Optional: LLM in Detective layer (fact extraction only, no opinions) ---
# AUDITOR_DETECTIVE_LLM=1   # DocAnalyst uses get_detective_llm() (or get_llm()) to classify theoretical depth

# --- LangSmith tracing (recommended for debugging) ---
# Set these BEFORE the app runs; .env is loaded at startup.
# LANGCHAIN_TRACING_V2=true
# LANGCHAIN_PROJECT=automaton-auditor
# LANGCHAIN_API_KEY=lsv2_pt_...  # required for traces to appear in LangSmith (get at smith.langchain.com)
